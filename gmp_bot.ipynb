{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5cf262792cc9dc35",
      "metadata": {
        "id": "5cf262792cc9dc35"
      },
      "source": [
        "<h1 align=\"center\"><font size=\"20\"> GMP OpenAI Assitant </font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8de529e8-3891-4f47-8585-65b92b80bbf7",
      "metadata": {
        "id": "8de529e8-3891-4f47-8585-65b92b80bbf7"
      },
      "source": [
        "\n",
        "\n",
        "<center><img src=\"images/chroma.webp\" width=\"80\" height=\"50\">   <img src=\"images/openai.png\" width=\"50\" height=\"50\"</center>   <img src=\"images/wordmark-langchain.png\" width=\"200\" height=\"100\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50acc65c-db1c-4871-a758-85b9db73bf9b",
      "metadata": {
        "id": "50acc65c-db1c-4871-a758-85b9db73bf9b"
      },
      "source": [
        " <h1 align=\"center\"><font size=\"4\"> History Aware OpenAI Assistant with RAG-Chroma and LangChain</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab909656-2b1a-4ff5-af9b-17c945a4c29d",
      "metadata": {
        "id": "ab909656-2b1a-4ff5-af9b-17c945a4c29d"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ad41cf424b3280",
      "metadata": {
        "id": "b8ad41cf424b3280"
      },
      "source": [
        "**Note**: If working in colab run the commands below in a code line:\n",
        "\n",
        "```\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain-community\n",
        "!pip install -U -q langchain-community pypdf\n",
        "!pip install -q gradio\n",
        "!pip install -q gradio-client\n",
        "!pip install -q chromadb\n",
        "!pip install -q langchain-chroma\n",
        "!pip install -q langchain-openai\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# comment out if running in colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfjaT8j-66J6",
        "outputId": "7bc3c56b-a4ce-4e87-ec23-8606dad4ad80"
      },
      "id": "JfjaT8j-66J6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/chatbot/openai-rag/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvYcKqFu8DI4",
        "outputId": "26c8aed8-aa2c-44c9-ac7b-4ec96d341ab8"
      },
      "id": "VvYcKqFu8DI4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/chatbot/openai-rag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "631de949-a373-44c4-8b6c-8174cfca7c54",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-12T13:36:26.613998Z",
          "start_time": "2025-03-12T13:36:26.597772Z"
        },
        "id": "631de949-a373-44c4-8b6c-8174cfca7c54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "import openai\n",
        "import chromadb\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "else:\n",
        "    openai.api_key  = os.environ['OPENAI_API_KEY']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If runing in colab**\n",
        "\n",
        "Comment lines 8-15 from the cell above and run:\n",
        "\n",
        "```os.environ[\"OPENAI_API_KEY\"] = \"Your key\" ```\n",
        "\n"
      ],
      "metadata": {
        "id": "mRTInuMsFInW"
      },
      "id": "mRTInuMsFInW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b2f9c6-e3a5-41e1-a85b-9ea7fd7a2476",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-12T13:36:30.144558Z",
          "start_time": "2025-03-12T13:36:30.137668Z"
        },
        "id": "f3b2f9c6-e3a5-41e1-a85b-9ea7fd7a2476"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains import (RetrievalQA,  ConversationalRetrievalChain,\n",
        "                    LLMChain, StuffDocumentsChain)\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb914ba59e3666d3",
      "metadata": {
        "id": "fb914ba59e3666d3"
      },
      "source": [
        "### Paths to local database and RAG directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76463ce1-e102-47ae-a85c-5abe2975a04e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-12T13:36:31.894034Z",
          "start_time": "2025-03-12T13:36:31.889504Z"
        },
        "id": "76463ce1-e102-47ae-a85c-5abe2975a04e"
      },
      "outputs": [],
      "source": [
        "# Path to pdf files\n",
        "file_path = \"validation\"\n",
        "# Path to save vectorized documents\n",
        "persist_directory='vectors'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b917685f31c0e8be",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-12T13:36:34.120659Z",
          "start_time": "2025-03-12T13:36:33.896038Z"
        },
        "id": "b917685f31c0e8be"
      },
      "outputs": [],
      "source": [
        "# Create the embeddings and models\n",
        "models = ['o3-mini', 'gpt-4o' ]\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "# Note if using o3-mini remove the temperature parameter\n",
        "llm = ChatOpenAI(model=models[1], temperature=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402648400d8a2883",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-12T12:45:54.799903Z",
          "start_time": "2025-03-12T12:45:54.796486Z"
        },
        "id": "402648400d8a2883"
      },
      "outputs": [],
      "source": [
        "# Create a prompt to instruct the model how to interact in a brief manner.\n",
        "condense_template =(\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ebc4476205035e1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-12T12:45:56.975575Z",
          "start_time": "2025-03-12T12:45:56.970803Z"
        },
        "id": "2ebc4476205035e1"
      },
      "outputs": [],
      "source": [
        "# Create a prompt to instruct the model how to interact.\n",
        "system_prompt = (\"\"\"\n",
        "You are an AI compliance advisor system, and provide answers to questions by using fact based information when possible. You are well versed in Good Manufacturing Practices (GMP) and validation in life science industry. You can help provide insights about critical test for validation and draft validation and qualification protocols based on the context.\n",
        "Use the following pieces of information to provide a concise answer to user questions {input}.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "The response should be specific.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c0dada16f740903",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-12T12:46:04.552581Z",
          "start_time": "2025-03-12T12:46:04.543495Z"
        },
        "id": "8c0dada16f740903"
      },
      "outputs": [],
      "source": [
        "def rag_conversation(pdf_path, k, embeddings, model=llm, general_instruction=condense_template, system_instruction=system_prompt):\n",
        "\n",
        "    # load documents pdf records from a directory\n",
        "    loader = PyPDFDirectoryLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # split documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    chunked_documents = text_splitter.split_documents(docs)\n",
        "\n",
        "    # create vector database from data\n",
        "    client = chromadb.Client()\n",
        "\n",
        "    if client.list_collections():\n",
        "        collection = client.create_collection(\"validation\")\n",
        "    else:\n",
        "        print(\"Collection already exists\")\n",
        "    vectordb = Chroma.from_documents(\n",
        "\n",
        "        documents=chunked_documents,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "\n",
        "    # define retriever\n",
        "    retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
        "\n",
        "    condense_instruction = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", general_instruction),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    history_aware_retriever = create_history_aware_retriever(model, retriever,\n",
        "                                                             condense_instruction)\n",
        "\n",
        "    qa_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_instruction),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        "    )\n",
        "    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "    convo_qa_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
        "\n",
        "\n",
        "    return convo_qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7628dc730c4159c5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-12T12:46:14.349334Z",
          "start_time": "2025-03-12T12:46:09.615398Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7628dc730c4159c5",
        "outputId": "88228774-10e1-442d-b534-339c669c3b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection already exists\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the vector database\n",
        "rag_chain = rag_conversation(file_path, 3, embeddings, llm, condense_template, system_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdb527b3bd0b7101",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-12T12:46:19.782871Z",
          "start_time": "2025-03-12T12:46:15.952579Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "fdb527b3bd0b7101",
        "outputId": "11196c39-07bb-4b89-d184-ca316c2d7c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ee9e60b9b6a402c0a2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ee9e60b9b6a402c0a2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://2d7a6044e2fd5ab1fa.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://ee9e60b9b6a402c0a2.gradio.live\n"
          ]
        }
      ],
      "source": [
        "share = False\n",
        "chat_history = []\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    # Your app code here\n",
        "    gr.Markdown(\"\"\" <h1><center>OpenAI GMP-Assistant</center></h1>\n",
        "    <br>\n",
        "    Ask your compliance assistant about GMP!\"\"\")\n",
        "    chatbot = gr.Chatbot(type=\"messages\", height=300)\n",
        "    msg = gr.Textbox()\n",
        "\n",
        "    with gr.Row():\n",
        "        submit = gr.Button(\"Submit\", variant=\"primary\")\n",
        "        clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "        def respond(input, chat_history):\n",
        "            response = rag_chain.invoke({\"input\":input, \"chat_history\": chat_history})\n",
        "            chat_history.append({\"role\": \"user\", \"content\": input})\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": response['answer']})\n",
        "            return \"\", chat_history\n",
        "\n",
        "    submit.click(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "# Display Chatbot in URL running publicly for 72 hr\n",
        "if share:\n",
        "    demo.launch(share=share)\n",
        "\n",
        "# Launches locally - debug to run in colab\n",
        "else:\n",
        "    demo.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "citation-manager": {
      "items": {}
    },
    "kernelspec": {
      "display_name": "Python (bedrockAI)",
      "language": "python",
      "name": "bedrockai"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}